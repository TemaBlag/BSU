{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786f0600-2207-4c60-ba83-60b91536d301",
   "metadata": {},
   "source": [
    "# Libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef545582-c38f-426b-adc5-16858b5ad730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip uninstall -y numpy scipy scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb97cb7-f661-439e-94d5-3df99ee85500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"numpy<2.0\" \"scipy==1.11.4\" \"scikit-learn==1.2.2\" pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a34dfa-ebb5-4b04-9808-e21c469a1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fe4527-6c2d-4798-bc79-e1854603087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pytdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bee0062f-7433-420b-81b4-e51328b45082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Union, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import AllChem, rdFingerprintGenerator\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tdc.single_pred import ADME, Tox\n",
    "from tdc.generation import MolGen\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_add_pool\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error, average_precision_score\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GCNConv, global_add_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d95f3f63-730e-46f5-bc1c-a3d601f67b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e5c7b17-aae9-4f33-9383-104666f7d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85293007-ff30-4002-8c33-27cfee6f158e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c919e6b-6e0a-4883-afb8-c8c472dcdb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESTokenizer:\n",
    "    \"\"\"\n",
    "    Regex-based tokenizer for chemical formulas.\n",
    "    Essential for Sequence models (CNN, Mamba).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file: str = None, max_len: int = 128):\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.sos_token = \"<sos>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        self.token_pattern = re.compile(r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|\"\n",
    "            \"P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\")\n",
    "        \n",
    "        if vocab_file:\n",
    "            self.load_vocab(vocab_file)\n",
    "        else:\n",
    "            self.vocab = {\n",
    "                self.pad_token: 0,\n",
    "                self.sos_token: 1,\n",
    "                self.eos_token: 2,\n",
    "                self.unk_token: 3\n",
    "            }\n",
    "            self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def train(self, smiles_list: List[str]):\n",
    "        \"\"\"Build vocabulary from a list of SMILES.\"\"\"\n",
    "        unique_tokens = set()\n",
    "        for smi in smiles_list:\n",
    "            tokens = self.token_pattern.findall(smi)\n",
    "            unique_tokens.update(tokens)\n",
    "        \n",
    "        start_idx = len(self.vocab)\n",
    "        for i, token in enumerate(sorted(unique_tokens)):\n",
    "            self.vocab[token] = start_idx + i\n",
    "            \n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        logger.info(f\"Tokenizer trained. Vocab size: {len(self.vocab)}\")\n",
    "\n",
    "    def encode(self, smiles: str) -> torch.Tensor:\n",
    "        \"\"\"SMILES -> LongTensor [max_len]\"\"\"\n",
    "        tokens = self.token_pattern.findall(smiles)\n",
    "        ids = [self.vocab.get(t, self.vocab[self.unk_token]) for t in tokens]\n",
    "        ids = [self.vocab[self.sos_token]] + ids + [self.vocab[self.eos_token]]\n",
    "        \n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.vocab[self.pad_token]] * (self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len-1] + [self.vocab[self.eos_token]]\n",
    "            \n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def save_vocab(self, path: str):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.vocab, f)\n",
    "\n",
    "    def load_vocab(self, path: str):\n",
    "        with open(path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8d11cd5-0fe4-4b9a-a2e0-cff8f07235aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolFeaturizer:\n",
    "    \"\"\"Factory class to generate different molecular representations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def smiles_to_morgan(smiles: str, radius: int = 2, n_bits: int = 2048) -> torch.Tensor:\n",
    "        \"\"\"Generates Morgan Fingerprint (ECFP).\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.zeros(n_bits)\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "        arr = np.zeros((0,), dtype=np.int8)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def smiles_to_graph(smiles: str) -> Data:\n",
    "        \"\"\"\n",
    "        Generates PyTorch Geometric Data object.\n",
    "        Simple node features: AtomicNum (one-hot or integer).\n",
    "        \"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return Data(x=torch.zeros((1, 1)), edge_index=torch.zeros((2, 0)))\n",
    "        atom_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_features.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x = torch.tensor(atom_features, dtype=torch.long).unsqueeze(1) # [N, 1]\n",
    "        rows, cols = [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start = bond.GetBeginAtomIdx()\n",
    "            end = bond.GetEndAtomIdx()\n",
    "            rows.extend([start, end])\n",
    "            cols.extend([end, start])\n",
    "            \n",
    "        edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84f703f9-905c-4185-96c3-996e7699b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularDataset(Dataset):\n",
    "    def __init__(self, smiles_list, labels, featurizer):\n",
    "        \"\"\"\n",
    "        Обновленный класс: принимает featurizer (функцию/объект), \n",
    "        а не строку modality.\n",
    "        \"\"\"\n",
    "        self.smiles = smiles_list\n",
    "        self.labels = labels\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smi = self.smiles[idx]\n",
    "        label = self.labels[idx]\n",
    "        x = self.featurizer(smi)\n",
    "        if label is None or np.isnan(label): \n",
    "            label = 0.0 \n",
    "            \n",
    "        y = torch.tensor([label], dtype=torch.float32)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb1dadea-f1e8-4676-8ae7-0ef762bfb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "TOKENIZER_PATH = os.path.join(DATA_DIR, \"tokenizer.json\")\n",
    "MAX_SEQ_LEN = 128  # Максимальная длина SMILES\n",
    "\n",
    "\n",
    "\n",
    "def ensure_dirs():\n",
    "    if not os.path.exists(RAW_DIR):\n",
    "        os.makedirs(RAW_DIR)\n",
    "    if not os.path.exists(PROCESSED_DIR):\n",
    "        os.makedirs(PROCESSED_DIR)\n",
    "\n",
    "def process_and_download_all():\n",
    "    ensure_dirs()\n",
    "    \n",
    "    logger.info(\"Downloading ZINC for pretraining...\")\n",
    "    data = MolGen(name='ZINC', path=RAW_DIR) \n",
    "    split = data.get_split()\n",
    "    \n",
    "    train_smiles = split['train']['smiles'][:250000].tolist()\n",
    "    \n",
    "    logger.info(f\"ZINC loaded. Using {len(train_smiles)} molecules for tokenizer training.\")\n",
    "\n",
    "    logger.info(\"Training Tokenizer...\")\n",
    "    tokenizer = SMILESTokenizer(max_len=MAX_SEQ_LEN)\n",
    "    tokenizer.train(train_smiles)\n",
    "    tokenizer.save_vocab(TOKENIZER_PATH)\n",
    "    logger.info(f\"Tokenizer saved to {TOKENIZER_PATH}\")\n",
    "    logger.info(f\"Vocab size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "    tasks = {\n",
    "        'Caco2_Wang': 'Caco2',\n",
    "        'HIA_Hou': 'HIA',\n",
    "        'Pgp_Broccatelli': 'Pgp',\n",
    "        'Bioavailability_Ma': 'Bioav',\n",
    "        'Lipophilicity_AstraZeneca': 'Lipo',\n",
    "        'Solubility_AqSolDB': 'AqSol'\n",
    "    }\n",
    "    \n",
    "    for tdc_name, paper_name in tasks.items():\n",
    "        logger.info(f\"Downloading downstream task: {paper_name} ({tdc_name})...\")\n",
    "        try:\n",
    "            data = ADME(name=tdc_name, path=RAW_DIR)\n",
    "            split = data.get_split(method='scaffold', seed=42, frac=[0.8, 0.1, 0.1])\n",
    "            \n",
    "            save_task_dir = os.path.join(PROCESSED_DIR, paper_name)\n",
    "            os.makedirs(save_task_dir, exist_ok=True)\n",
    "            \n",
    "            split['train'].to_csv(os.path.join(save_task_dir, 'train.csv'), index=False)\n",
    "            split['val'].to_csv(os.path.join(save_task_dir, 'val.csv'), index=False)\n",
    "            split['test'].to_csv(os.path.join(save_task_dir, 'test.csv'), index=False)\n",
    "            \n",
    "            logger.info(f\"Task {paper_name} processed and saved to {save_task_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading {paper_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2277e804-cee7-4895-8cc3-0adb9a4d24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_and_download_all()\n",
    "# logger.info(\"All data prepared successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b43628-9433-414c-afc7-ad4806d589fe",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "111e790d-d393-4770-ae3b-3834189734e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorganFeaturizer:\n",
    "    def __init__(self, radius=2, n_bits=1024):\n",
    "        self.generator = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "        self.n_bits = n_bits\n",
    "        \n",
    "    def __call__(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None: return torch.zeros(self.n_bits)\n",
    "        return torch.tensor(self.generator.GetFingerprintAsNumPy(mol), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87b9ec01-e888-424e-bf18-77e158e36046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeaturizer:\n",
    "    def __call__(self, smiles: str):\n",
    "        if not HAS_PYG: return None\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            # Заглушка тоже должна быть (1, 9)\n",
    "            return Data(x=torch.zeros((1, 9)), edge_index=torch.empty((2, 0), dtype=torch.long))\n",
    "\n",
    "        # Атомы\n",
    "        atom_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            # Базовые 5 фич\n",
    "            feats = [\n",
    "                atom.GetAtomicNum(),\n",
    "                atom.GetDegree(),\n",
    "                atom.GetExplicitValence(),\n",
    "                int(atom.GetIsAromatic()),\n",
    "                atom.GetFormalCharge()\n",
    "            ]\n",
    "            \n",
    "            # !!! ИСПРАВЛЕНИЕ: Дополняем нулями до 9, чтобы совпадало с GCN(node_features=9) !!!\n",
    "            padding = [0] * (9 - len(feats))\n",
    "            feats += padding\n",
    "            \n",
    "            atom_features.append(feats)\n",
    "            \n",
    "        x = torch.tensor(atom_features, dtype=torch.float32)\n",
    "\n",
    "        # Связи\n",
    "        rows, cols = [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start = bond.GetBeginAtomIdx()\n",
    "            end = bond.GetEndAtomIdx()\n",
    "            rows.extend([start, end])\n",
    "            cols.extend([end, start])\n",
    "\n",
    "        if len(rows) == 0:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05d5d197-f6a4-4a8a-a1b9-3bc1eec32886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    Modality 3: Sequence (for SMILES-CNN, Mamba)\n",
    "    Character-level tokenization.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len: int = 128, vocab_path: Optional[str] = None):\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.sos_token = \"<sos>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        \n",
    "        base_chars = [\n",
    "            'C', 'c', 'O', 'o', 'N', 'n', 'S', 's',\n",
    "            '=', '#', '(', ')', '[', ']', '1', '2', '3', \n",
    "            '+', '-', '.', 'F', 'Cl', 'Br', 'I', 'H'\n",
    "        ]\n",
    "        \n",
    "        self.vocab = {\n",
    "            self.pad_token: 0,\n",
    "            self.sos_token: 1,\n",
    "            self.eos_token: 2,\n",
    "            self.unk_token: 3\n",
    "        }\n",
    "        \n",
    "        for char in base_chars:\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = len(self.vocab)\n",
    "                \n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        if vocab_path:\n",
    "            self.load_vocab(vocab_path)\n",
    "\n",
    "    def fit_on_zinc(self, smiles_list: List[str]):\n",
    "        \"\"\"\n",
    "        Scans ZINC dataset to add missing characters (e.g., '4', '5', 'P', '/', '\\')\n",
    "        to the vocabulary to ensure coverage.\n",
    "        \"\"\"\n",
    "        unique_chars = set()\n",
    "        for smi in smiles_list:\n",
    "            unique_chars.update(list(smi))\n",
    "            \n",
    "        for char in sorted(list(unique_chars)):\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = len(self.vocab)\n",
    "        \n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        print(f\"Tokenizer fitted. Vocab size: {len(self.vocab)}\")\n",
    "\n",
    "    def encode(self, smiles: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process: String -> List[Int] -> Pad/Truncate -> Tensor\n",
    "        \"\"\"\n",
    "        chars = list(smiles)\n",
    "        \n",
    "        ids = [self.vocab.get(c, self.vocab[self.unk_token]) for c in chars]\n",
    "        \n",
    "        ids = [self.vocab[self.sos_token]] + ids + [self.vocab[self.eos_token]]\n",
    "        \n",
    "        if len(ids) < self.max_len:\n",
    "            padding = [self.vocab[self.pad_token]] * (self.max_len - len(ids))\n",
    "            ids = ids + padding\n",
    "        else:\n",
    "            ids = ids[:self.max_len - 1] + [self.vocab[self.eos_token]]\n",
    "            \n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def save_vocab(self, path: str):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.vocab, f)\n",
    "            \n",
    "    def load_vocab(self, path: str):\n",
    "        with open(path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.inverse_vocab = {v: k for k, v in self.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae51572d-c895-41f8-b2fc-7227fc768782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morgan Shape: torch.Size([1024])\n",
      "Graph Nodes: torch.Size([14, 9])\n",
      "Graph Edges: torch.Size([2, 30])\n",
      "Tokens Shape: torch.Size([128])\n",
      "Tokens: tensor([ 1,  4,  8, 18,  4, 12,  8,  4, 19, 12,  4, 18,  4, 14, 12])\n"
     ]
    }
   ],
   "source": [
    "smi = \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"\n",
    "\n",
    "morgan = MorganFeaturizer(radius=2, n_bits=1024)\n",
    "print(f\"Morgan Shape: {morgan(smi).shape}\")\n",
    "\n",
    "graph_gen = GraphFeaturizer()\n",
    "graph_data = graph_gen(smi)\n",
    "print(f\"Graph Nodes: {graph_data.x.shape}\")\n",
    "print(f\"Graph Edges: {graph_data.edge_index.shape}\")\n",
    "\n",
    "tokenizer = CharTokenizer(max_len=128)\n",
    "tokens = tokenizer.encode(smi)\n",
    "print(f\"Tokens Shape: {tokens.shape}\")\n",
    "print(f\"Tokens: {tokens[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fa90e-2441-4724-89b8-449d08030610",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f193ced9-262f-4c48-9339-5a741a52b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \"\"\"Base class with utility to count parameters.\"\"\"\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54c1f1cf-63ae-404e-925a-0d5130b96446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Baseline 1: Morgan + MLP\n",
    "# ==========================================\n",
    "class MorganMLP(BaseModel):\n",
    "    def __init__(self, input_dim=1024, output_dim=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbd07698-06f3-4be8-b2bc-3b7f8f198f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MorganMLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MorganMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3d61130-6db7-4e38-852b-9e52ea1d4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Baseline 2: SMILES + CNN\n",
    "# ==========================================\n",
    "class SmilesCNN(BaseModel):\n",
    "    def __init__(self, vocab_size, embed_dim=64, output_dim=1, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=6)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=96, kernel_size=8)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(96, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = torch.max(x, dim=2)[0]\n",
    "        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "caf5128f-d40b-4ca8-9a0c-17250a1a5372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmilesCNN(\n",
       "  (embedding): Embedding(64, 64, padding_idx=0)\n",
       "  (conv1): Conv1d(64, 32, kernel_size=(4,), stride=(1,))\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(6,), stride=(1,))\n",
       "  (conv3): Conv1d(64, 96, kernel_size=(8,), stride=(1,))\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=96, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SmilesCNN(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "127986f2-85bb-45ee-beb5-5678f63669b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Baseline 3: GCN\n",
    "# ==========================================\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, node_features=9, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([GCNConv(node_features, hidden_dim)])\n",
    "        for _ in range(4): self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1de32aa1-18b6-40bf-93e9-dafae532f4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (convs): ModuleList(\n",
       "    (0): GCNConv(9, 100)\n",
       "    (1-4): 4 x GCNConv(100, 100)\n",
       "  )\n",
       "  (head): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88584c47-78eb-4b21-af7d-ffbd20d87902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Baseline 4: NeuralFP (Frozen Encoder)\n",
    "# ==========================================\n",
    "class NeuralFP(BaseModel):\n",
    "    def __init__(self, pretrained_gcn: GCN, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.encoder = pretrained_gcn\n",
    "        self.encoder.head = nn.Identity()\n",
    "        \n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        with torch.no_grad():\n",
    "            embedding = self.encoder(data)\n",
    "\n",
    "        return self.decoder(embedding)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b00a69eb-0a49-4c47-bb07-d4267442c5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralFP(\n",
       "  (encoder): GCN(\n",
       "    (convs): ModuleList(\n",
       "      (0): GCNConv(9, 100)\n",
       "      (1-4): 4 x GCNConv(100, 100)\n",
       "    )\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralFP(GCN())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bed0a-6bda-4b1e-a1a8-ab3fb4248262",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52cd183f-3590-432c-88a8-94955ced6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalTrainer:\n",
    "    def __init__(self, model, params, task_config, device):\n",
    "        self.model = model.to(device)\n",
    "        self.task_type = task_config['type']\n",
    "        self.metric_name = task_config['metric']\n",
    "        self.device = device\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "        self.criterion = nn.BCEWithLogitsLoss() if self.task_type == 'classification' else nn.L1Loss()\n",
    "\n",
    "    def run(self, train, val, test, epochs):\n",
    "        train_l = get_dataloader(train, 32, True)\n",
    "        val_l = get_dataloader(val, 32, False)\n",
    "        test_l = get_dataloader(test, 32, False)\n",
    "        \n",
    "        higher_is_better = self.metric_name in ['roc-auc', 'pr-auc', 'spearman']\n",
    "        best_score = -float('inf') if higher_is_better else float('inf')\n",
    "        best_state = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for batch in tqdm(train_l, desc=f\"Ep {epoch+1}\", leave=False):\n",
    "                self.optimizer.zero_grad()\n",
    "                if isinstance(batch, list): x, y = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                else: x, y = batch.to(self.device), batch.y.to(self.device)\n",
    "                \n",
    "                pred = self.model(x)\n",
    "                if y.shape!=pred.shape: y=y.view_as(pred)\n",
    "                loss = self.criterion(pred, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            val_score = self.evaluate(val_l)\n",
    "            improved = (val_score > best_score) if higher_is_better else (val_score < best_score)\n",
    "            if improved:\n",
    "                best_score = val_score\n",
    "                best_state = self.model.state_dict()\n",
    "        if best_state: self.model.load_state_dict(best_state)\n",
    "        return self.evaluate(test_l)\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        preds, targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch, list): x, y = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                else: x, y = batch.to(self.device), batch.y.to(self.device)\n",
    "                out = self.model(x)\n",
    "                preds.extend(out.cpu().numpy())\n",
    "                targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        preds = np.array(preds).flatten()\n",
    "        targets = np.array(targets).flatten()\n",
    "        \n",
    "        if np.isnan(preds).any(): preds = np.nan_to_num(preds)\n",
    "\n",
    "        if self.metric_name == 'roc-auc':\n",
    "            try:\n",
    "                probs = 1 / (1 + np.exp(-preds))\n",
    "                if len(np.unique(targets)) < 2: return 0.5\n",
    "                return roc_auc_score(targets, probs)\n",
    "            except: return 0.5\n",
    "        elif self.metric_name == 'pr-auc':\n",
    "            try:\n",
    "                probs = 1 / (1 + np.exp(-preds))\n",
    "                return average_precision_score(targets, probs)\n",
    "            except: return 0.0\n",
    "        elif self.metric_name == 'mae':\n",
    "            return mean_absolute_error(targets, preds)\n",
    "        elif self.metric_name == 'spearman':\n",
    "            val, _ = spearmanr(targets, preds)\n",
    "            return 0.0 if np.isnan(val) else val\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b3ad79a-3ee6-4185-86f1-b6993f2bb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFactory:\n",
    "    @staticmethod\n",
    "    def create_model(model_name, config, device, tokenizer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: One of ['morgan_mlp', 'cnn', 'gcn', 'neural_fp', 'mamba']\n",
    "            config: The dict from config.yaml\n",
    "            device: torch.device\n",
    "            tokenizer: Instance of CharTokenizer/SMILESTokenizer (needed for seq models)\n",
    "        Returns:\n",
    "            model (nn.Module), modality_type (str)\n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name == 'morgan_mlp':\n",
    "            input_dim = config['featurization']['morgan_nbits']\n",
    "            model = MorganMLP(input_dim=input_dim)\n",
    "            return model, 'morgan'\n",
    "\n",
    "        elif model_name == 'cnn':\n",
    "            if tokenizer is None:\n",
    "                raise ValueError(\"Tokenizer required for CNN initialization\")\n",
    "            \n",
    "            vocab_size = len(tokenizer.vocab)\n",
    "            model = SmilesCNN(\n",
    "                vocab_size=vocab_size,\n",
    "                embed_dim=64,\n",
    "                max_len=config['featurization']['max_seq_len']\n",
    "            )\n",
    "            return model, 'seq'\n",
    "\n",
    "        elif model_name == 'gcn':\n",
    "            model = GCN(\n",
    "                node_features=6, \n",
    "                hidden_dim=config['model_params']['gnn_hidden_dim']\n",
    "            )\n",
    "            return model, 'graph'\n",
    "\n",
    "        elif model_name == 'neural_fp':\n",
    "            base_gcn = GCN(node_features=6, hidden_dim=config['model_params']['gnn_hidden_dim'])\n",
    "            \n",
    "            model = NeuralFP(pretrained_gcn=base_gcn)\n",
    "            return model, 'graph'\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ccb740ad-05f3-405e-bb04-634dda39ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS_CONFIG = {\n",
    "    # --- Distribution (Table 2) ---\n",
    "    'BBB':      {'tdc_name': 'BBB_Martins',       'type': 'classification', 'metric': 'roc-auc'},\n",
    "    'PPBR':     {'tdc_name': 'PPBR_AZ',           'type': 'regression',     'metric': 'mae'},\n",
    "    'VD':       {'tdc_name': 'VDss_Lombardo',     'type': 'regression',     'metric': 'mae'},\n",
    "\n",
    "    # --- Metabolism (Table 3) ---\n",
    "    'CYP2D6-I': {'tdc_name': 'CYP2D6_Veith',      'type': 'classification', 'metric': 'pr-auc'},\n",
    "    'CYP3A4-I': {'tdc_name': 'CYP3A4_Veith',      'type': 'classification', 'metric': 'pr-auc'},\n",
    "    'CYP2C9-I': {'tdc_name': 'CYP2C9_Veith',      'type': 'classification', 'metric': 'pr-auc'},\n",
    "    \n",
    "    'CYP2D6-S': {'tdc_name': 'CYP2D6_Substrate_CarbonMangels', 'type': 'classification', 'metric': 'pr-auc'},\n",
    "    'CYP3A4-S': {'tdc_name': 'CYP3A4_Substrate_CarbonMangels', 'type': 'classification', 'metric': 'roc-auc'},\n",
    "    'CYP2C9-S': {'tdc_name': 'CYP2C9_Substrate_CarbonMangels', 'type': 'classification', 'metric': 'pr-auc'},\n",
    "\n",
    "    # --- Pharmacokinetics ---\n",
    "    'Half-Life':{'tdc_name': 'Half_Life_Obach',        'type': 'regression', 'metric': 'spearman'},\n",
    "    'CL-Micro': {'tdc_name': 'Clearance_Microsome_AZ', 'type': 'regression', 'metric': 'spearman'},\n",
    "    'CL-Hepa':  {'tdc_name': 'Clearance_Hepatocyte_AZ','type': 'regression', 'metric': 'spearman'},\n",
    "\n",
    "    # --- Toxicity (Table 4 & 1) ---\n",
    "    'hERG':     {'tdc_name': 'hERG',         'type': 'classification', 'metric': 'roc-auc'},\n",
    "    'AMES':     {'tdc_name': 'AMES',         'type': 'classification', 'metric': 'roc-auc'},\n",
    "    'DILI':     {'tdc_name': 'DILI',         'type': 'classification', 'metric': 'roc-auc'},\n",
    "    'LD50':     {'tdc_name': 'LD50_Zhu',     'type': 'regression',     'metric': 'mae'},\n",
    "    \n",
    "    # --- Absorption (Table 1) ---\n",
    "    'Caco2':    {'tdc_name': 'Caco2_Wang',   'type': 'regression',     'metric': 'mae'},\n",
    "    'HIA':      {'tdc_name': 'HIA_Hou',      'type': 'classification', 'metric': 'roc-auc'},\n",
    "    'Pgp':      {'tdc_name': 'Pgp_Broccatelli','type': 'classification', 'metric': 'roc-auc'},\n",
    "    'Bioav':    {'tdc_name': 'Bioavailability_Ma','type': 'classification', 'metric': 'roc-auc'},\n",
    "    'Lipo':     {'tdc_name': 'Lipophilicity_AstraZeneca', 'type': 'regression', 'metric': 'mae'},\n",
    "    'AqSol':    {'tdc_name': 'Solubility_AqSolDB', 'type': 'regression', 'metric': 'mae'},\n",
    "}\n",
    "\n",
    "CFG = {\n",
    "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'data_path': './data/raw',\n",
    "    'training': {'learning_rate': 1e-3, 'epochs': 10} \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c5e8237-9a2c-413f-958b-1272283210a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(cfg_entry):\n",
    "    name = cfg_entry['tdc_name']\n",
    "    try: data = ADME(name=name, path=CFG['data_path'])\n",
    "    except: data = Tox(name=name, path=CFG['data_path'])\n",
    "    s = data.get_split(method='scaffold', seed=42, frac=[0.8, 0.1, 0.1])\n",
    "    return s['train'], s['valid'], s['test']\n",
    "\n",
    "def main():\n",
    "    device = torch.device(CFG['device'])\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    tok = CharTokenizer()\n",
    "    morgan = MorganFeaturizer()\n",
    "    graph = GraphFeaturizer()\n",
    "    \n",
    "    models = ['morgan_mlp', 'cnn', 'gcn', 'neural_fp']\n",
    "    \n",
    "    target_tasks = list(TASKS_CONFIG.keys())\n",
    "    \n",
    "    all_results = {}\n",
    "\n",
    "    print(f\"Tasks scheduled: {target_tasks}\")\n",
    "\n",
    "    for task in target_tasks:\n",
    "        t_cfg = TASKS_CONFIG[task]\n",
    "        metric_display = t_cfg['metric'].upper()\n",
    "        logger.info(f\"\\n{'='*50}\\nTask: {task} ({t_cfg['tdc_name']}) | Metric: {metric_display}\\n{'='*50}\")\n",
    "        \n",
    "        all_results[task] = {}\n",
    "\n",
    "        try:\n",
    "            train, val, test = get_data(t_cfg)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {task}: {e}\"); continue\n",
    "\n",
    "        for m_name in models:\n",
    "            logger.info(f\"Training {m_name}...\")\n",
    "            \n",
    "            if m_name == 'morgan_mlp':\n",
    "                feat = morgan; model = MorganMLP()\n",
    "            elif m_name == 'cnn':\n",
    "                feat = lambda x: tok.encode(x); model = SmilesCNN(len(tok.vocab))\n",
    "            elif m_name in ['gcn', 'neural_fp']:\n",
    "                if not HAS_PYG: \n",
    "                    all_results[task][m_name] = None\n",
    "                    continue\n",
    "                feat = graph; base = GCN()\n",
    "                model = base if m_name=='gcn' else NeuralFP(base)\n",
    "            \n",
    "            tr_ds = MolecularDataset(train['Drug'].tolist(), train['Y'].tolist(), feat)\n",
    "            va_ds = MolecularDataset(val['Drug'].tolist(), val['Y'].tolist(), feat)\n",
    "            te_ds = MolecularDataset(test['Drug'].tolist(), test['Y'].tolist(), feat)\n",
    "            \n",
    "            trainer = UniversalTrainer(model, CFG['training'], t_cfg, device)\n",
    "            final_metric = trainer.run(tr_ds, va_ds, te_ds, CFG['training']['epochs'])\n",
    "            \n",
    "            all_results[task][m_name] = final_metric\n",
    "            logger.info(f\">>> {task} | {m_name} Test {metric_display}: {final_metric:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"FINAL RESULTS SUMMARY (Best Test Metrics)\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    header = f\"{'Task':<15} | {'Metric':<9} | \" + \" | \".join([f\"{m:>10}\" for m in models])\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for task in target_tasks:\n",
    "        if task not in all_results: continue\n",
    "        \n",
    "        metric_name = TASKS_CONFIG[task]['metric'].upper()\n",
    "        row_str = f\"{task:<15} | {metric_name:<9} | \"\n",
    "        \n",
    "        for m in models:\n",
    "            val = all_results[task].get(m, None)\n",
    "            if val is None:\n",
    "                val_str = \"   N/A    \"\n",
    "            else:\n",
    "                val_str = f\"{val:10.4f}\"\n",
    "            row_str += f\"{val_str} | \"\n",
    "        print(row_str)\n",
    "        \n",
    "    print(\"=\"*85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d0b74f9-57f1-4215-8c97-4cced4193262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:41:13,081 - INFO - Device: cpu\n",
      "2025-12-09 21:41:13,083 - INFO - \n",
      "==================================================\n",
      "Task: BBB (BBB_Martins) | Metric: ROC-AUC\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks scheduled: ['BBB', 'PPBR', 'VD', 'CYP2D6-I', 'CYP3A4-I', 'CYP2C9-I', 'CYP2D6-S', 'CYP3A4-S', 'CYP2C9-S', 'Half-Life', 'CL-Micro', 'CL-Hepa', 'hERG', 'AMES', 'DILI', 'LD50', 'Caco2', 'HIA', 'Pgp', 'Bioav', 'Lipo', 'AqSol']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2030/2030 [00:01<00:00, 1862.99it/s]\n",
      "2025-12-09 21:41:14,190 - INFO - Training morgan_mlp...\n",
      "2025-12-09 21:41:31,091 - INFO - >>> BBB | morgan_mlp Test ROC-AUC: 0.8429      \n",
      "2025-12-09 21:41:31,091 - INFO - Training cnn...\n",
      "2025-12-09 21:41:44,230 - INFO - >>> BBB | cnn Test ROC-AUC: 0.9058             \n",
      "2025-12-09 21:41:44,231 - INFO - Training gcn...\n",
      "2025-12-09 21:42:06,338 - INFO - >>> BBB | gcn Test ROC-AUC: 0.4588             \n",
      "2025-12-09 21:42:06,339 - INFO - Training neural_fp...\n",
      "2025-12-09 21:42:24,894 - INFO - >>> BBB | neural_fp Test ROC-AUC: 0.7388       \n",
      "2025-12-09 21:42:24,895 - INFO - \n",
      "==================================================\n",
      "Task: PPBR (PPBR_AZ) | Metric: MAE\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|███████████████████████████████████████| 265k/265k [00:03<00:00, 85.6kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 1614/1614 [00:00<00:00, 1664.59it/s]\n",
      "2025-12-09 21:42:32,609 - INFO - Training morgan_mlp...\n",
      "2025-12-09 21:42:48,080 - INFO - >>> PPBR | morgan_mlp Test MAE: 11.9905        \n",
      "2025-12-09 21:42:48,081 - INFO - Training cnn...\n",
      "2025-12-09 21:42:58,496 - INFO - >>> PPBR | cnn Test MAE: 10.4078               \n",
      "2025-12-09 21:42:58,496 - INFO - Training gcn...\n",
      "2025-12-09 21:43:18,261 - INFO - >>> PPBR | gcn Test MAE: 15.9136               \n",
      "2025-12-09 21:43:18,261 - INFO - Training neural_fp...\n",
      "2025-12-09 21:43:34,022 - INFO - >>> PPBR | neural_fp Test MAE: 16.9043         \n",
      "2025-12-09 21:43:34,022 - INFO - \n",
      "==================================================\n",
      "Task: VD (VDss_Lombardo) | Metric: MAE\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 89.9k/89.9k [00:01<00:00, 69.8kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 1130/1130 [00:00<00:00, 1568.02it/s]\n",
      "2025-12-09 21:43:38,712 - INFO - Training morgan_mlp...\n",
      "2025-12-09 21:43:48,807 - INFO - >>> VD | morgan_mlp Test MAE: 2.4211           \n",
      "2025-12-09 21:43:48,808 - INFO - Training cnn...\n",
      "2025-12-09 21:43:55,339 - INFO - >>> VD | cnn Test MAE: 2.3123                  \n",
      "2025-12-09 21:43:55,339 - INFO - Training gcn...\n",
      "2025-12-09 21:44:08,715 - INFO - >>> VD | gcn Test MAE: 2.5624                  \n",
      "2025-12-09 21:44:08,716 - INFO - Training neural_fp...\n",
      "2025-12-09 21:44:19,469 - INFO - >>> VD | neural_fp Test MAE: 2.4784            \n",
      "2025-12-09 21:44:19,470 - INFO - \n",
      "==================================================\n",
      "Task: CYP2D6-I (CYP2D6_Veith) | Metric: PR-AUC\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|████████████████████████████████████████| 800k/800k [00:06<00:00, 116kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████| 13130/13130 [00:06<00:00, 1937.90it/s]\n",
      "2025-12-09 21:44:36,690 - INFO - Training morgan_mlp...\n",
      "2025-12-09 21:46:25,816 - INFO - >>> CYP2D6-I | morgan_mlp Test PR-AUC: 0.5425  \n",
      "2025-12-09 21:46:25,817 - INFO - Training cnn...\n",
      "2025-12-09 21:47:54,763 - INFO - >>> CYP2D6-I | cnn Test PR-AUC: 0.6008         \n",
      "2025-12-09 21:47:54,763 - INFO - Training gcn...\n",
      "2025-12-09 21:50:54,295 - INFO - >>> CYP2D6-I | gcn Test PR-AUC: 0.1764         \n",
      "2025-12-09 21:50:54,296 - INFO - Training neural_fp...\n",
      "2025-12-09 21:53:02,573 - INFO - >>> CYP2D6-I | neural_fp Test PR-AUC: 0.2612   \n",
      "2025-12-09 21:53:02,573 - INFO - \n",
      "==================================================\n",
      "Task: CYP3A4-I (CYP3A4_Veith) | Metric: PR-AUC\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|████████████████████████████████████████| 746k/746k [00:05<00:00, 126kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████| 12328/12328 [00:06<00:00, 1929.86it/s]\n",
      "2025-12-09 21:53:27,477 - INFO - Training morgan_mlp...\n",
      "2025-12-09 21:55:36,956 - INFO - >>> CYP3A4-I | morgan_mlp Test PR-AUC: 0.7866  \n",
      "2025-12-09 21:55:36,957 - INFO - Training cnn...\n",
      "2025-12-09 21:57:03,954 - INFO - >>> CYP3A4-I | cnn Test PR-AUC: 0.7958         \n",
      "2025-12-09 21:57:03,954 - INFO - Training gcn...\n",
      "2025-12-09 21:59:39,190 - INFO - >>> CYP3A4-I | gcn Test PR-AUC: 0.5398         \n",
      "2025-12-09 21:59:39,191 - INFO - Training neural_fp...\n",
      "2025-12-09 22:01:29,965 - INFO - >>> CYP3A4-I | neural_fp Test PR-AUC: 0.5812   \n",
      "2025-12-09 22:01:29,966 - INFO - \n",
      "==================================================\n",
      "Task: CYP2C9-I (CYP2C9_Veith) | Metric: PR-AUC\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|███████████████████████████████████████| 740k/740k [00:09<00:00, 82.0kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████| 12092/12092 [00:06<00:00, 1857.96it/s]\n",
      "2025-12-09 22:01:48,746 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:03:28,918 - INFO - >>> CYP2C9-I | morgan_mlp Test PR-AUC: 0.7242  \n",
      "2025-12-09 22:03:28,918 - INFO - Training cnn...\n",
      "2025-12-09 22:04:38,263 - INFO - >>> CYP2C9-I | cnn Test PR-AUC: 0.6923         \n",
      "2025-12-09 22:04:38,263 - INFO - Training gcn...\n",
      "2025-12-09 22:06:44,534 - INFO - >>> CYP2C9-I | gcn Test PR-AUC: 0.5775         \n",
      "2025-12-09 22:06:44,534 - INFO - Training neural_fp...\n",
      "2025-12-09 22:08:32,365 - INFO - >>> CYP2C9-I | neural_fp Test PR-AUC: 0.5417   \n",
      "2025-12-09 22:08:32,366 - INFO - \n",
      "==================================================\n",
      "Task: CYP2D6-S (CYP2D6_Substrate_CarbonMangels) | Metric: PR-AUC\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 45.4k/45.4k [00:00<00:00, 59.1kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 667/667 [00:00<00:00, 1872.51it/s]\n",
      "2025-12-09 22:08:36,532 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:08:41,983 - INFO - >>> CYP2D6-S | morgan_mlp Test PR-AUC: 0.7373  \n",
      "2025-12-09 22:08:41,984 - INFO - Training cnn...\n",
      "2025-12-09 22:08:45,876 - INFO - >>> CYP2D6-S | cnn Test PR-AUC: 0.5490         \n",
      "2025-12-09 22:08:45,876 - INFO - Training gcn...\n",
      "2025-12-09 22:08:52,545 - INFO - >>> CYP2D6-S | gcn Test PR-AUC: 0.3902         \n",
      "2025-12-09 22:08:52,546 - INFO - Training neural_fp...\n",
      "2025-12-09 22:08:58,553 - INFO - >>> CYP2D6-S | neural_fp Test PR-AUC: 0.4028   \n",
      "2025-12-09 22:08:58,554 - INFO - \n",
      "==================================================\n",
      "Task: CYP3A4-S (CYP3A4_Substrate_CarbonMangels) | Metric: ROC-AUC\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 46.0k/46.0k [00:00<00:00, 68.0kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 670/670 [00:00<00:00, 1834.68it/s]\n",
      "2025-12-09 22:09:02,051 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:09:07,694 - INFO - >>> CYP3A4-S | morgan_mlp Test ROC-AUC: 0.7239 \n",
      "2025-12-09 22:09:07,694 - INFO - Training cnn...\n",
      "2025-12-09 22:09:11,667 - INFO - >>> CYP3A4-S | cnn Test ROC-AUC: 0.6355        \n",
      "2025-12-09 22:09:11,668 - INFO - Training gcn...\n",
      "2025-12-09 22:09:18,558 - INFO - >>> CYP3A4-S | gcn Test ROC-AUC: 0.5769        \n",
      "2025-12-09 22:09:18,559 - INFO - Training neural_fp...\n",
      "2025-12-09 22:09:24,410 - INFO - >>> CYP3A4-S | neural_fp Test ROC-AUC: 0.5545  \n",
      "2025-12-09 22:09:24,411 - INFO - \n",
      "==================================================\n",
      "Task: CYP2C9-S (CYP2C9_Substrate_CarbonMangels) | Metric: PR-AUC\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 45.6k/45.6k [00:00<00:00, 46.4kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 669/669 [00:00<00:00, 1843.09it/s]\n",
      "2025-12-09 22:09:28,419 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:09:34,028 - INFO - >>> CYP2C9-S | morgan_mlp Test PR-AUC: 0.4223  \n",
      "2025-12-09 22:09:34,028 - INFO - Training cnn...\n",
      "2025-12-09 22:09:37,996 - INFO - >>> CYP2C9-S | cnn Test PR-AUC: 0.3883         \n",
      "2025-12-09 22:09:37,997 - INFO - Training gcn...\n",
      "2025-12-09 22:09:44,756 - INFO - >>> CYP2C9-S | gcn Test PR-AUC: 0.3713         \n",
      "2025-12-09 22:09:44,757 - INFO - Training neural_fp...\n",
      "2025-12-09 22:09:50,639 - INFO - >>> CYP2C9-S | neural_fp Test PR-AUC: 0.3680   \n",
      "2025-12-09 22:09:50,640 - INFO - \n",
      "==================================================\n",
      "Task: Half-Life (Half_Life_Obach) | Metric: SPEARMAN\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 53.6k/53.6k [00:01<00:00, 47.6kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 667/667 [00:00<00:00, 1563.72it/s]\n",
      "2025-12-09 22:09:55,669 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:10:01,505 - INFO - >>> Half-Life | morgan_mlp Test SPEARMAN: 0.6276\n",
      "2025-12-09 22:10:01,506 - INFO - Training cnn...\n",
      "2025-12-09 22:10:05,385 - INFO - >>> Half-Life | cnn Test SPEARMAN: 0.0472      \n",
      "2025-12-09 22:10:05,386 - INFO - Training gcn...\n",
      "2025-12-09 22:10:12,823 - INFO - >>> Half-Life | gcn Test SPEARMAN: 0.1115      \n",
      "2025-12-09 22:10:12,824 - INFO - Training neural_fp...\n",
      "2025-12-09 22:10:19,052 - INFO - >>> Half-Life | neural_fp Test SPEARMAN: -0.1117\n",
      "2025-12-09 22:10:19,052 - INFO - \n",
      "==================================================\n",
      "Task: CL-Micro (Clearance_Microsome_AZ) | Metric: SPEARMAN\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|██████████████████████████████████████| 81.7k/81.7k [00:00<00:00, 111kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 1102/1102 [00:00<00:00, 1660.15it/s]\n",
      "2025-12-09 22:10:24,294 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:10:34,007 - INFO - >>> CL-Micro | morgan_mlp Test SPEARMAN: 0.4243\n",
      "2025-12-09 22:10:34,007 - INFO - Training cnn...\n",
      "2025-12-09 22:10:40,403 - INFO - >>> CL-Micro | cnn Test SPEARMAN: 0.1770       \n",
      "2025-12-09 22:10:40,404 - INFO - Training gcn...\n",
      "2025-12-09 22:10:53,833 - INFO - >>> CL-Micro | gcn Test SPEARMAN: 0.0974       \n",
      "2025-12-09 22:10:53,834 - INFO - Training neural_fp...\n",
      "2025-12-09 22:11:05,056 - INFO - >>> CL-Micro | neural_fp Test SPEARMAN: 0.0004 \n",
      "2025-12-09 22:11:05,057 - INFO - \n",
      "==================================================\n",
      "Task: CL-Hepa (Clearance_Hepatocyte_AZ) | Metric: SPEARMAN\n",
      "==================================================\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 91.6k/91.6k [00:01<00:00, 59.8kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 1213/1213 [00:00<00:00, 1647.80it/s]\n",
      "2025-12-09 22:11:10,241 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:11:21,006 - INFO - >>> CL-Hepa | morgan_mlp Test SPEARMAN: 0.2577 \n",
      "2025-12-09 22:11:21,006 - INFO - Training cnn...\n",
      "2025-12-09 22:11:28,054 - INFO - >>> CL-Hepa | cnn Test SPEARMAN: 0.2452        \n",
      "2025-12-09 22:11:28,054 - INFO - Training gcn...\n",
      "2025-12-09 22:11:42,095 - INFO - >>> CL-Hepa | gcn Test SPEARMAN: -0.0030       \n",
      "2025-12-09 22:11:42,096 - INFO - Training neural_fp...\n",
      "2025-12-09 22:11:54,576 - INFO - >>> CL-Hepa | neural_fp Test SPEARMAN: -0.0504 \n",
      "2025-12-09 22:11:54,576 - INFO - \n",
      "==================================================\n",
      "Task: hERG (hERG) | Metric: ROC-AUC\n",
      "==================================================\n",
      "['lipophilicity_astrazeneca', 'solubility_aqsoldb', 'hydrationfreeenergy_freesolv', 'caco2_wang', 'pampa_ncats', 'approved_pampa_ncats', 'hia_hou', 'pgp_broccatelli', 'bioavailability_ma', 'vdss_lombardo', 'cyp2c19_veith', 'cyp2d6_veith', 'cyp3a4_veith', 'cyp1a2_veith', 'cyp2c9_veith', 'cyp2c9_substrate_carbonmangels', 'cyp2d6_substrate_carbonmangels', 'cyp3a4_substrate_carbonmangels', 'bbb_martins', 'b3db_classification', 'b3db_regression', 'ppbr_az', 'half_life_obach', 'clearance_hepatocyte_az', 'clearance_microsome_az', 'hlm', 'rlm']\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 50.2k/50.2k [00:00<00:00, 62.4kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 655/655 [00:00<00:00, 1687.31it/s]\n",
      "2025-12-09 22:11:58,491 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:12:04,368 - INFO - >>> hERG | morgan_mlp Test ROC-AUC: 0.7350     \n",
      "2025-12-09 22:12:04,368 - INFO - Training cnn...\n",
      "2025-12-09 22:12:08,324 - INFO - >>> hERG | cnn Test ROC-AUC: 0.8506            \n",
      "2025-12-09 22:12:08,324 - INFO - Training gcn...\n",
      "2025-12-09 22:12:15,679 - INFO - >>> hERG | gcn Test ROC-AUC: 0.6931            \n",
      "2025-12-09 22:12:15,679 - INFO - Training neural_fp...\n",
      "2025-12-09 22:12:22,027 - INFO - >>> hERG | neural_fp Test ROC-AUC: 0.6456      \n",
      "2025-12-09 22:12:22,028 - INFO - \n",
      "==================================================\n",
      "Task: AMES (AMES) | Metric: ROC-AUC\n",
      "==================================================\n",
      "['lipophilicity_astrazeneca', 'solubility_aqsoldb', 'hydrationfreeenergy_freesolv', 'caco2_wang', 'pampa_ncats', 'approved_pampa_ncats', 'hia_hou', 'pgp_broccatelli', 'bioavailability_ma', 'vdss_lombardo', 'cyp2c19_veith', 'cyp2d6_veith', 'cyp3a4_veith', 'cyp1a2_veith', 'cyp2c9_veith', 'cyp2c9_substrate_carbonmangels', 'cyp2d6_substrate_carbonmangels', 'cyp3a4_substrate_carbonmangels', 'bbb_martins', 'b3db_classification', 'b3db_regression', 'ppbr_az', 'half_life_obach', 'clearance_hepatocyte_az', 'clearance_microsome_az', 'hlm', 'rlm']\n",
      "Downloading...\n",
      "100%|████████████████████████████████████████| 344k/344k [00:02<00:00, 119kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 7278/7278 [00:02<00:00, 2790.82it/s]\n",
      "2025-12-09 22:12:30,586 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:13:22,882 - INFO - >>> AMES | morgan_mlp Test ROC-AUC: 0.7880     \n",
      "2025-12-09 22:13:22,882 - INFO - Training cnn...\n",
      "2025-12-09 22:14:08,813 - INFO - >>> AMES | cnn Test ROC-AUC: 0.8103            \n",
      "2025-12-09 22:14:08,814 - INFO - Training gcn...\n",
      "2025-12-09 22:15:06,618 - INFO - >>> AMES | gcn Test ROC-AUC: 0.6800            \n",
      "2025-12-09 22:15:06,619 - INFO - Training neural_fp...\n",
      "2025-12-09 22:15:54,284 - INFO - >>> AMES | neural_fp Test ROC-AUC: 0.6212      \n",
      "2025-12-09 22:15:54,285 - INFO - \n",
      "==================================================\n",
      "Task: DILI (DILI) | Metric: ROC-AUC\n",
      "==================================================\n",
      "['lipophilicity_astrazeneca', 'solubility_aqsoldb', 'hydrationfreeenergy_freesolv', 'caco2_wang', 'pampa_ncats', 'approved_pampa_ncats', 'hia_hou', 'pgp_broccatelli', 'bioavailability_ma', 'vdss_lombardo', 'cyp2c19_veith', 'cyp2d6_veith', 'cyp3a4_veith', 'cyp1a2_veith', 'cyp2c9_veith', 'cyp2c9_substrate_carbonmangels', 'cyp2d6_substrate_carbonmangels', 'cyp3a4_substrate_carbonmangels', 'bbb_martins', 'b3db_classification', 'b3db_regression', 'ppbr_az', 'half_life_obach', 'clearance_hepatocyte_az', 'clearance_microsome_az', 'hlm', 'rlm']\n",
      "Downloading...\n",
      "100%|█████████████████████████████████████| 26.7k/26.7k [00:00<00:00, 27.2kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 475/475 [00:00<00:00, 2022.96it/s]\n",
      "2025-12-09 22:15:59,337 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:16:03,245 - INFO - >>> DILI | morgan_mlp Test ROC-AUC: 0.8229     \n",
      "2025-12-09 22:16:03,246 - INFO - Training cnn...\n",
      "2025-12-09 22:16:05,819 - INFO - >>> DILI | cnn Test ROC-AUC: 0.8837            \n",
      "2025-12-09 22:16:05,820 - INFO - Training gcn...\n",
      "2025-12-09 22:16:10,262 - INFO - >>> DILI | gcn Test ROC-AUC: 0.7483            \n",
      "2025-12-09 22:16:10,263 - INFO - Training neural_fp...\n",
      "2025-12-09 22:16:13,983 - INFO - >>> DILI | neural_fp Test ROC-AUC: 0.8264      \n",
      "2025-12-09 22:16:13,984 - INFO - \n",
      "==================================================\n",
      "Task: LD50 (LD50_Zhu) | Metric: MAE\n",
      "==================================================\n",
      "['lipophilicity_astrazeneca', 'solubility_aqsoldb', 'hydrationfreeenergy_freesolv', 'caco2_wang', 'pampa_ncats', 'approved_pampa_ncats', 'hia_hou', 'pgp_broccatelli', 'bioavailability_ma', 'vdss_lombardo', 'cyp2c19_veith', 'cyp2d6_veith', 'cyp3a4_veith', 'cyp1a2_veith', 'cyp2c9_veith', 'cyp2c9_substrate_carbonmangels', 'cyp2d6_substrate_carbonmangels', 'cyp3a4_substrate_carbonmangels', 'bbb_martins', 'b3db_classification', 'b3db_regression', 'ppbr_az', 'half_life_obach', 'clearance_hepatocyte_az', 'clearance_microsome_az', 'hlm', 'rlm']\n",
      "Downloading...\n",
      "100%|████████████████████████████████████████| 707k/707k [00:03<00:00, 209kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 7385/7385 [00:02<00:00, 3147.99it/s]\n",
      "2025-12-09 22:16:22,566 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:17:13,180 - INFO - >>> LD50 | morgan_mlp Test MAE: 0.5645         \n",
      "2025-12-09 22:17:13,181 - INFO - Training cnn...\n",
      "2025-12-09 22:17:53,556 - INFO - >>> LD50 | cnn Test MAE: 0.5680                \n",
      "2025-12-09 22:17:53,556 - INFO - Training gcn...\n",
      "2025-12-09 22:18:48,539 - INFO - >>> LD50 | gcn Test MAE: 0.7237                \n",
      "2025-12-09 22:18:48,540 - INFO - Training neural_fp...\n",
      "2025-12-09 22:19:34,346 - INFO - >>> LD50 | neural_fp Test MAE: 0.7192          \n",
      "2025-12-09 22:19:34,347 - INFO - \n",
      "==================================================\n",
      "Task: Caco2 (Caco2_Wang) | Metric: MAE\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 910/910 [00:00<00:00, 1504.05it/s]\n",
      "2025-12-09 22:19:34,967 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:19:42,854 - INFO - >>> Caco2 | morgan_mlp Test MAE: 0.4822        \n",
      "2025-12-09 22:19:42,854 - INFO - Training cnn...\n",
      "2025-12-09 22:19:47,891 - INFO - >>> Caco2 | cnn Test MAE: 0.7722               \n",
      "2025-12-09 22:19:47,892 - INFO - Training gcn...\n",
      "2025-12-09 22:19:58,772 - INFO - >>> Caco2 | gcn Test MAE: 1.0998               \n",
      "2025-12-09 22:19:58,772 - INFO - Training neural_fp...\n",
      "2025-12-09 22:20:08,672 - INFO - >>> Caco2 | neural_fp Test MAE: 1.0300         \n",
      "2025-12-09 22:20:08,673 - INFO - \n",
      "==================================================\n",
      "Task: HIA (HIA_Hou) | Metric: ROC-AUC\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 578/578 [00:00<00:00, 1693.68it/s]\n",
      "2025-12-09 22:20:09,029 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:20:14,260 - INFO - >>> HIA | morgan_mlp Test ROC-AUC: 0.8045      \n",
      "2025-12-09 22:20:14,261 - INFO - Training cnn...\n",
      "2025-12-09 22:20:18,315 - INFO - >>> HIA | cnn Test ROC-AUC: 0.7985             \n",
      "2025-12-09 22:20:18,315 - INFO - Training gcn...\n",
      "2025-12-09 22:20:25,138 - INFO - >>> HIA | gcn Test ROC-AUC: 0.7652             \n",
      "2025-12-09 22:20:25,139 - INFO - Training neural_fp...\n",
      "2025-12-09 22:20:30,916 - INFO - >>> HIA | neural_fp Test ROC-AUC: 0.6182       \n",
      "2025-12-09 22:20:30,917 - INFO - \n",
      "==================================================\n",
      "Task: Pgp (Pgp_Broccatelli) | Metric: ROC-AUC\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 1218/1218 [00:00<00:00, 1426.93it/s]\n",
      "2025-12-09 22:20:31,787 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:20:43,089 - INFO - >>> Pgp | morgan_mlp Test ROC-AUC: 0.8897      \n",
      "2025-12-09 22:20:43,090 - INFO - Training cnn...\n",
      "2025-12-09 22:20:51,023 - INFO - >>> Pgp | cnn Test ROC-AUC: 0.8817             \n",
      "2025-12-09 22:20:51,024 - INFO - Training gcn...\n",
      "2025-12-09 22:21:06,042 - INFO - >>> Pgp | gcn Test ROC-AUC: 0.7401             \n",
      "2025-12-09 22:21:06,043 - INFO - Training neural_fp...\n",
      "2025-12-09 22:21:18,779 - INFO - >>> Pgp | neural_fp Test ROC-AUC: 0.8108       \n",
      "2025-12-09 22:21:18,779 - INFO - \n",
      "==================================================\n",
      "Task: Bioav (Bioavailability_Ma) | Metric: ROC-AUC\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|███████████████████████████████████████| 640/640 [00:00<00:00, 1636.50it/s]\n",
      "2025-12-09 22:21:19,186 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:21:25,235 - INFO - >>> Bioav | morgan_mlp Test ROC-AUC: 0.5469    \n",
      "2025-12-09 22:21:25,235 - INFO - Training cnn...\n",
      "2025-12-09 22:21:30,313 - INFO - >>> Bioav | cnn Test ROC-AUC: 0.5612           \n",
      "2025-12-09 22:21:30,314 - INFO - Training gcn...\n",
      "2025-12-09 22:21:39,224 - INFO - >>> Bioav | gcn Test ROC-AUC: 0.5833           \n",
      "2025-12-09 22:21:39,225 - INFO - Training neural_fp...\n",
      "2025-12-09 22:21:46,455 - INFO - >>> Bioav | neural_fp Test ROC-AUC: 0.5182     \n",
      "2025-12-09 22:21:46,456 - INFO - \n",
      "==================================================\n",
      "Task: Lipo (Lipophilicity_AstraZeneca) | Metric: MAE\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 4200/4200 [00:03<00:00, 1322.47it/s]\n",
      "2025-12-09 22:21:49,663 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:22:34,824 - INFO - >>> Lipo | morgan_mlp Test MAE: 0.6952         \n",
      "2025-12-09 22:22:34,825 - INFO - Training cnn...\n",
      "2025-12-09 22:23:02,573 - INFO - >>> Lipo | cnn Test MAE: 0.7462                \n",
      "2025-12-09 22:23:02,574 - INFO - Training gcn...\n",
      "2025-12-09 22:24:01,378 - INFO - >>> Lipo | gcn Test MAE: 1.0099                \n",
      "2025-12-09 22:24:01,379 - INFO - Training neural_fp...\n",
      "2025-12-09 22:25:09,431 - INFO - >>> Lipo | neural_fp Test MAE: 0.9703          \n",
      "2025-12-09 22:25:09,432 - INFO - \n",
      "==================================================\n",
      "Task: AqSol (Solubility_AqSolDB) | Metric: MAE\n",
      "==================================================\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "100%|█████████████████████████████████████| 9982/9982 [00:04<00:00, 2106.33it/s]\n",
      "2025-12-09 22:25:14,224 - INFO - Training morgan_mlp...\n",
      "2025-12-09 22:26:32,895 - INFO - >>> AqSol | morgan_mlp Test MAE: 1.1097        \n",
      "2025-12-09 22:26:32,895 - INFO - Training cnn...\n",
      "2025-12-09 22:27:38,020 - INFO - >>> AqSol | cnn Test MAE: 0.9871               \n",
      "2025-12-09 22:27:38,021 - INFO - Training gcn...\n",
      "2025-12-09 22:29:22,844 - INFO - >>> AqSol | gcn Test MAE: 1.3727               \n",
      "2025-12-09 22:29:22,845 - INFO - Training neural_fp...\n",
      "2025-12-09 22:30:52,064 - INFO - >>> AqSol | neural_fp Test MAE: 1.4733         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================================================\n",
      "FINAL RESULTS SUMMARY (Best Test Metrics)\n",
      "=====================================================================================\n",
      "Task            | Metric    | morgan_mlp |        cnn |        gcn |  neural_fp\n",
      "-------------------------------------------------------------------------------\n",
      "BBB             | ROC-AUC   |     0.8429 |     0.9058 |     0.4588 |     0.7388 | \n",
      "PPBR            | MAE       |    11.9905 |    10.4078 |    15.9136 |    16.9043 | \n",
      "VD              | MAE       |     2.4211 |     2.3123 |     2.5624 |     2.4784 | \n",
      "CYP2D6-I        | PR-AUC    |     0.5425 |     0.6008 |     0.1764 |     0.2612 | \n",
      "CYP3A4-I        | PR-AUC    |     0.7866 |     0.7958 |     0.5398 |     0.5812 | \n",
      "CYP2C9-I        | PR-AUC    |     0.7242 |     0.6923 |     0.5775 |     0.5417 | \n",
      "CYP2D6-S        | PR-AUC    |     0.7373 |     0.5490 |     0.3902 |     0.4028 | \n",
      "CYP3A4-S        | ROC-AUC   |     0.7239 |     0.6355 |     0.5769 |     0.5545 | \n",
      "CYP2C9-S        | PR-AUC    |     0.4223 |     0.3883 |     0.3713 |     0.3680 | \n",
      "Half-Life       | SPEARMAN  |     0.6276 |     0.0472 |     0.1115 |    -0.1117 | \n",
      "CL-Micro        | SPEARMAN  |     0.4243 |     0.1770 |     0.0974 |     0.0004 | \n",
      "CL-Hepa         | SPEARMAN  |     0.2577 |     0.2452 |    -0.0030 |    -0.0504 | \n",
      "hERG            | ROC-AUC   |     0.7350 |     0.8506 |     0.6931 |     0.6456 | \n",
      "AMES            | ROC-AUC   |     0.7880 |     0.8103 |     0.6800 |     0.6212 | \n",
      "DILI            | ROC-AUC   |     0.8229 |     0.8837 |     0.7483 |     0.8264 | \n",
      "LD50            | MAE       |     0.5645 |     0.5680 |     0.7237 |     0.7192 | \n",
      "Caco2           | MAE       |     0.4822 |     0.7722 |     1.0998 |     1.0300 | \n",
      "HIA             | ROC-AUC   |     0.8045 |     0.7985 |     0.7652 |     0.6182 | \n",
      "Pgp             | ROC-AUC   |     0.8897 |     0.8817 |     0.7401 |     0.8108 | \n",
      "Bioav           | ROC-AUC   |     0.5469 |     0.5612 |     0.5833 |     0.5182 | \n",
      "Lipo            | MAE       |     0.6952 |     0.7462 |     1.0099 |     0.9703 | \n",
      "AqSol           | MAE       |     1.1097 |     0.9871 |     1.3727 |     1.4733 | \n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
